# VIP Assesment ETL â€” from Excel files â†’ PostgreSQL (default) or DuckDB (for simplicity)

An idempotent **ETL pipeline** that reads two Excel files (doctors, appointments), cleans/transforms them, and loads the results into:
- **PostgreSQL** (default; via Docker or your local server), or
- **DuckDB** (no server required) with `--duckdb`.

It also writes final cleaned CSVs and includes business SQL queries.

## ğŸ§° Tech Stack

- **Python** (pandas, openpyxl, python-dotenv)
- **PostgreSQL** (via Docker or local)
- **DuckDB** (embedded; single file)
- **Logging** to console and `logs/etl.log`

---

## ğŸ“¦ Repository Layout

healthtech-etl/
â”œâ”€ etl/
â”‚ â”œâ”€ extract.py # read Excel sources
â”‚ â”œâ”€ transform.py # normalize/clean data
â”‚ â”œâ”€ load.py # PostgreSQL loader (TRUNCATE/LOAD)
â”‚ â”œâ”€ load_duckdb.py # DuckDB loader (CREATE OR REPLACE TABLE)
â”‚ â”œâ”€ logging_utils.py # console + file logs
â”‚ â””â”€ config.py # paths, env vars, backends
â”œâ”€ data/
â”‚ â”œâ”€ raw/
â”‚ â”‚ â”œâ”€ Data Enginner's Doctors Excel - VIP Medical Group.xlsx
â”‚ â”‚ â””â”€ Data Engineer's Appointments Excel - VIP Medical Group.xlsx
â”‚ â””â”€ processed/ # output CSVs
â”œâ”€ logs/ # etl.log written here
â”œâ”€ queries.sql # business questions
â”œâ”€ docker-compose.yml # optional local Postgres (14)
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â””â”€ main.py

## ğŸš€ Quick Start

```bash
git clone https://github.com/your-org/sales-etl.git
cd VIP_Test

python -m venv .venv
# macOS/Linux:
source .venv/bin/activate
# Windows PowerShell:
# .venv\Scripts\Activate.ps1

pip install -r requirements.txt
```
**(VERY IMPORTANT)** I have Include two possible ways to run the ETL

First one with PostgreSQL integration (No Dockerized)

```bash
cp .env.example .env
# .env defaults:
# PGHOST=localhost
# PGPORT=5432
# PGUSER=postgres
# PGPASSWORD=postgres
# PGDATABASE=postgres
# PGSCHEMA=healthtech

# Run ETL (PostgreSQL backend)
python main.py
```

Second one (Recommended) Run with DuckDB (no server required)
```bash
# Run ETL (DuckDB backend)
python main.py --duckdb
```

## ğŸ“¥ Inputs  ğŸ“¤ Outputs

- **Inputs** (place in data/raw/):

    Data Enginner's Doctors Excel - VIP Medical Group.xlsx (sheet: doctors)

    Data Engineer's Appointments Excel - VIP Medical Group.xlsx (sheet: appointments)

- **Outputs:**

    Final datasets (CSV): data/processed/doctors_clean.csv, data/processed/appointments_clean.csv

- **Database:**

    Postgres: schema healthtech, tables doctors, appointments

    DuckDB: file healthtech.duckdb with schema healthtech and the same tables

- **Logs:** logs/etl.log

## Questions

ğŸ”¢ Q1 â€” Which doctor has the most confirmed appointments? **ANSWER 152**

![image alt](https://github.com/diefgonzalezpac/Technical_Assesment_VIP/blob/4e50da54e98833b9b40df405006a13bfae4a15f4/Query%20Question%201.png)

ğŸ§â€â™€ï¸ Q2 â€” How many confirmed appointments does the patient with patient_id â€˜34â€™ have? **ANSWER 14**

![image alt](https://github.com/diefgonzalezpac/Technical_Assesment_VIP/blob/4e50da54e98833b9b40df405006a13bfae4a15f4/Query%20Question%202.png)

ğŸ—“ï¸ Q3 â€” How many cancelled appointments are there between Oct 21, 2025 and Oct 24, 2025 (inclusive)? **ANSWER 66**

![image alt](https://github.com/diefgonzalezpac/Technical_Assesment_VIP/blob/4e50da54e98833b9b40df405006a13bfae4a15f4/Query%20Question%203.png)

ğŸ§‘â€âš•ï¸ Q4 â€” What is the total number of confirmed appointments for each doctor? **ANSWER See table below**

![image alt](https://github.com/diefgonzalezpac/Technical_Assesment_VIP/blob/4e50da54e98833b9b40df405006a13bfae4a15f4/Query%20Question%204.png)

## AWS PROD ETL

-- **Orchestration:** 

    Amazon MWAA (Managed Airflow)
    Centralized DAGs, retries, SLAs, dependency management, auditability.

-- **Extract (E):** 

    AWS Lambda on S3 events or scheduled by Airflow
    Validates uploads, unzips/xlsxâ†’Parquet, basic normalization, pushes to S3 raw/bronze.

-- **Transform (T):**

    ELT in Snowflake using dbt (models, tests, docs) and/or SnowflakeOperator from Airflow. This keeps logic closer to the data and scales with Snowflakeâ€™s compute.

-- **Load (L):**

    As discussed in our last meeting Snowflake is the companyâ€™s warehouse
    Airflow â†’ SnowflakeOperator: run COPY INTO on a schedule with file manifests 

## Why these tools?

-- **MWAA (Airflow):** Standard orchestration, rich operator ecosystem (Snowflake, S3, Lambda, Glue), operational visibility, retries and also because of the kind of data, we have medical records about appointments that could be included day by day using cron operations, Airflow is an amaizing tool for this kind of scenarios

-- **Lambda (Extraction):** Serverless, cheap for light/medium workload; perfect for parsing Excel â†’ Parquet and metadata validation. Seconds to minutes runtime, for simplicity.

-- **Snowflake + DBT:(Transformation + Load):** Because of the little amount of data services like AWS Glue could be too much, light dbt operations inside Snowflake could be used for data modeling, testing, and table creations simplifying the operation
